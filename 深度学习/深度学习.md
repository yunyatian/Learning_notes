# 一.机器学习（深度学习）先导知识

## 1.机器学习（深度学习）的不同类型

> * Regression（回归分析）:输出的是一个数值，这做的是通过输入的各种条件，得出一个结果 。Regression的任务就是找出一个函数，使得可以通过这些输入得出最后的结果。
> * Classification（分类）：给出一些选项，输出其中正确的一个。Classification的任务就是找出一个函数，使得可以判断哪一个才是正确的选项。
> * Structured Learning（结构化学习）：使机器产生有结构的东西。简而言之就会说让机器学会创造这件事情。

## 2.机器怎么寻找函数？

> * 第一步，写出一个带有未知参数的函数。简单来说，就是猜测一下需要找的函数的数学公式是什么样的，假设函数为<font color='red'>$y=b+w*x$(回归模型)。其中y代表输出，x代表输入，w，b代表未知参数</font>。
>
> * 第二步，定义一个Loss，Loss实质上也是一个函数$L(w,b)$，这个函数的输入是上一步函数里的未知参数。Loss的输出代表这组参数输入好坏情况。Loss的计算需要训练集。<font color = 'red'>$L=（1/N）*\sum_n{e_n}$</font>，其中e代表的是比对训练集中的值得出的差值<font color = 'red'>$e=|y-\widehat{y}|$(MAE)</font>或<font color = 'red'>$e=(y-\widehat{y})^2$(MSE)</font>,其中$\widehat{y}$代表的是当前参数运算所得的值，y代表训练集中的实际值。<font color = 'red'>L越大，代表当前这组参数越差，L越小，代表当前这组参数越好。</font>
>
> * 第三步就是最佳化的问题，找一个w和b使得Loss的值最小，这就是最佳的答案 ，这需要用到Gradient Descent（梯度下降法）。
>
>   > * 先考虑一个参数的时候，随机选择一个值$w^0$，然后计算$\frac{\partial{L}}{\partial{w}}|_(w=w^0)$,即计算该点的导数（该点切线斜率），如果导数为负，则$w ^0$应该向右移动，如果导数为正，则$w^0 $应该向左移动，即<font color =  'red'>$w^0 = w^0 - \eta*\frac{\partial{L}}{\partial{w}}|_{w=w^0}$</font>，其中$\eta$是速率，由人为设置。
>   > * 通过上述步骤不断更新值，<font color = 'red'>直到达到最大更新次数，亦或者当出现求导值为0时，停止上述操作</font>>。找到所要找的当前最佳值$w ^0$。
>   > * <font color = 'red'>注意：此时找到的不一定是全局最优解，有可能会是局部最优解，因为一个函数可能不止一个极值点，而我们可能会因为初始的随机值而导致找到的随机值仅仅只是局部最优解。</font>
>   > * ![image-20211024160728427](D:\Learning_notes\深度学习\image-20211024160728427.png)
>   > * 两个参数的时候同一个参数同理，就是求两个参数的导数，进行上述的操作即可。
>
> * 以上的回归模型$y = b+w*x$仅仅只考虑了每一天的情况，是将每一天看作是一个整体在进行测试。但是往往有些时候单纯只考虑将某一天看作是一个整体是容易产生误差的。例如，预测博客明天的浏览量，因为周五，周六，周日的时候，因为放假因素会导致浏览量下降，但当以一天一天来看的话，这个规律不容易被发现，只有将一周当做一个整体，才能发现这个规律。所以回归模型可定义为<font color = 'red'>$y = b+\sum_{j=1}^{7}w_j*x_j$</font>，此时将一个星期看作一个整体，进行测试，去寻找最佳函数。
> * Linear Model过于简单，因为无论怎么更改，他都是一条直线，随着x的越来越大，y就越来越大。虽然可以设定不同的w，改变这条直线的斜率，也可以设定不同的b来改变直线和Y轴的交点，但是无论如何更改w和b，y都会随着x的增大而增大，你可以理解为博客前一天的浏览量很大，那么下一天的预测浏览量只会更大，但是现实情况并不是这个样子的。显然线性的回归模型有很大的局限性。下面的红色函数线无法生成，所以需要新的回归模型。
> * ![image-20211114172215262](D:\Learning_notes\深度学习\image-20211114172215262.png)
> * 红色的函数线可以通过<font color = 'red'>常数+函数集</font>获得，<font color ='red'>下列的函数图像可以通过函数0+函数1+函数2+函数3</font>获得。即使是曲线，也可以通过去取曲线上的一些点进行连线，然后像如下图进行构建函数。如果点取的够多，那么这条线性方程就会非常逼近曲线。
> * ![image-20211114190414178](D:\Learning_notes\深度学习\image-20211114190414178.png)
> * 下列蓝色的函数图像不一定能够直接写出来，可以使用Sigmoid（S型）函数曲线来逼近这个函数。Sigmoid函数式为<font color = 'red'>$y = c *\frac{1}{1+e^{-(b+w*x_1)}}=c*sigmoid(b+w*x_1)$</font>。
> * ![image-20211114191943318](D:\Learning_notes\深度学习\image-20211114191943318.png)
> * 对于Sigmoid函数，如果修改w，就会更改函数图像的斜率，如果修改b，就会使得图像左右移动，如果修改c，就会修改函数图像的最大值。
> * ![image-20211114193139682](D:\Learning_notes\深度学习\image-20211114193139682.png)
> * 通过更改w，b，c就可以制造出各个不同的Sigmoid函数，然后就可以去逼近各种不同的分段函数，然后各个不同的分段函数就可以叠加出需要的连续函数。所以之前的<font color = 'red'>函数1+函数2+函数3</font>就可以写成<font color ='red'>$c_1*sigmoid(b_1+w_1*x_1)+c_2*sigmoid(b_2+w_2*x_1)+c_3*sigmoid(b_3+w_3*x_1)$</font>。所以之前的函数式可以写为<font color ='red'>$y=b+\sum_i{c_i*sigmoid(b_i+w_i*x_1)}$</font>。
> * 上述方程考虑的是之前只考虑一天作为一个整体的情况，当我们需要将多个天数作为一个整体的时候，函数式应写为<font color = 'red'>$y=b+\sum_i{c_i}*sigmoid(b_i+\sum_j{w_{ij}*x_j})$</font>。其中可以令$r_i = b_i+\sum_j{w_{ij}*x_j}$。
> * ![image-20211114195528531](D:\Learning_notes\深度学习\image-20211114195528531.png)
> * 根据$r_i = b_i+\sum_j{w_{ij}*x_j}$，可以写出以下方程式：
>
> <center>r1 = b1 + w11 * x1 + w12 * x2 + w13 * x3<br/>
>     r2 = b2 + w21 * x1 + w22 * x2 + w23 * x3<br/>
>     r3 = b3 + w31 * x1 + w32 * x2 + w33 * x3</center>
>
> * 故可得：
>
> $$\left\{\begin{matrix}r_1\\r_2\\r_3\end{matrix}\right\}=\left\{\begin{matrix}b_1\\b_2\\b_3\end{matrix}\right\}+\left\{\begin{matrix}w_{11}&w_{12}&w_{13}\\w_{21}&w_{22}&w_{23}\\w_{31}&w_{32}&w_{33}\end{matrix}\right\}*\left\{\begin{matrix}x_1\\x_2\\x_3\end{matrix}\right\}$$.
>
> 即<font color = 'red'>$\vec r= \vec b + \vec W * \vec x$ </font>。
>
> <font color = 'red'>$y = b + \vec c^T * \vec a $</font>，其中<font color = 'red'>$\vec a = \sigma(\vec r)= sigmoid (\vec r)=sigmoid(\vec b + \vec W * \vec x)=\sigma(\vec b + \vec W * \vec x)$</font>。这其中x是已知数，剩下的$\vec W,\vec b,\vec c^T,b$都是未知参数，可以将这些未知参数放在一起用<font color = 'red'>$\vec {\theta}$</font>来表示。
>
> * 回归模型确定后，就可以通过之前说的Gradient Descent来进行测试，获得最佳参数，确定函数。
> * 除了可以用Sigmoid函数来逼近Hard Sigmoid，还可以使用Rectified Linear Unit(ReLu)来逼近该函数，<font color = 'red'>ReLu函数的表达式为$c*max(0,b+w*x_1)$</font>。所以需要使用两条ReLu函数来逼近Sigmoid函数，即$c*max(0,b+w*x_1)+c'*max(0,b'+w'*x_1)$。
> * ![image-20211114212223069](D:\Learning_notes\深度学习\image-20211114212223069.png)
> * 所以可以将Sigmoid函数的模型$y=b+\sum{_i}c_i*sigmoid(b_i+\sum{_j}w_{ij}*x_j)$转化为ReLu函数模型$y = b+\sum_{2*i}c_i*max (0,b_i+\sum_jw_{ij}*x_j)$。<font color = 'red'>切记，两个ReLu函数才能逼近一个Hard Sigmoid函数</font>。
> * Sigmoid函数和ReLu函数统称为Activation Function（激活函数）。
> * 接下来还可以对之前的模型进行更改。之前定义的模型中$\vec a= \sigma(\vec b + \vec W * \vec x)$，还可以将此时求得的$\vec a$再带入另一个函数式中，即<font color ='red'>$\vec a' = \sigma(\vec b'+\vec W'*\vec a)$</font>。还可以反复多做几次这种复用迭代，迭代次数由人为决定，次数越大，未知参数越多。
> * ![image-20211114215018272](D:\Learning_notes\深度学习\image-20211114215018272.png)
> * <font color = 'red'>每一个Sigmoid函数或者ReLu函数就是一个Neuron（神经元），每一排Neuron就被称为Hidden Layer(隐藏层)，有很多层的Hidden Layer的就叫做Deep。一整套技术就叫做Deep Learning（深度学习）</font>。
> * ![image-20211114215046002](D:\Learning_notes\深度学习\image-20211114215046002.png)
> * <font color = 'red'>并不是说隐藏层越多，我们的测试数据就会越精确，当隐藏层数过大时会出现Overfitting（过度拟合现象）</font>。
>

## 如何让函数更加优化

> * 首先检测训练集上的Loss 
>
> * 如果过大，
>
>   > * 可能是<font color = 'red'>model bias，即模型偏差、假设模型过于简单，导致最后产生的函数集合非常的小，这个集合中没有一个的解可以使得Loss变低。</font>在这种情况下，即使可以找到一个解，但这个解的Loss也只是在这些集合中是最小的，但实际上这个Loss还是很大。<font color = 'red'>解决办法：重新设计一个Model，给予这个Model更大的弹性。例如原来的函数为$y= b+w*x_1$，可以添加更多的特征点使得函数变为$y = b+\sum_{j=1}^{7}w_j*x_j$，或者是采用深度学习的方法$y=b+\sum_i{c_i}*sigmoid(b_i+\sum_j{w_{ij}*x_j})$</font>
>   > * 也可能是<font color = 'red'>Optimization做的不够好，即可能会出现Local minimal （局部最优解）问题。 </font>
>   > * 对于上述两个存在问题进行判断，
>   > * ![image-20211207191153831](D:\Learning_notes\深度学习\image-20211207191153831.png)

