### 卷积层：

假设输入图像大小为$W_1*H_1*D_1$,W,H,D分别代表输入图像的宽，高和通道数，其中卷积层中的卷积核大小为(F,F)，步长为S，填充大小为P，卷积核的个数为K，那么经过卷积层的输出图像大小宽，高和通道数分别为<font color='red'>   $W_2=\frac {W_1-F+2P}{S}+1,H_2=\frac {H_1-F+2P}{S}+1,D_2=K$</font>,其中$D_2=K$是因为卷积核实际大小是$F*F*D_1$，所以在进行卷积时是对整张图片进行卷积的，会对$D_1$个通道中的相对应位置的值进行卷积，然后进行相加得到一个值,然后将该值与偏置相加。所以K个卷积核在进行卷积后会生成K个通道数，即原先多个通道仅对应一个卷积核，不同的卷积核对同一张图片里的所有通道进行运算得到卷积核数目的通道数。

<font color = 'red'>卷积层参数大小为$F*F*D_1*K+K$（加K是因为需要为每一个卷积结果添加一个偏置，所以偏置数目应于卷积核数目相同）</font>

### 全连接层：

全连接层会将输入图片$W*H*C$转变为$1*M$的一维数据，然后将该数据与权重向量进行点乘操作，权重向量大小为$M*N$，点乘后的结果为$1*N$，该结果将作为激活函数的输入，最后激活函数的输出结果就是全连接层的最终输出结果。

###  池化层

池化是对图像进行压缩（下采样）的一种方法，有利于减少后续参数量，并且不会影响后续特征的提取。在<font color='red'>图像分类</font>领域应用广泛，但是在<font color='red'>目标检测</font>和<font color='red'>图像分割</font>存在感就比较弱一点。这是因为在目标检测和图像分割领域中需要<font color='red'>检测对象在图像中的具体位置信息，而池化层容易丢失这一信息</font>。<font color='red'>池化层没有要学习的参数，它只是从目标区域中取最大值或者平均值</font>；在池化运算中，<font color='red'>输入特征图和输出特征图的通道数不会发生变化（计算是按独立通道数进行的）</font>。

池化层对原始特征层的信息进行压缩，当输入数据发生微小偏差的时候，池化层的计算仍然会返回相同的结果，因此<font color='red'>池化层对数据发生的微小偏差具有一定的鲁棒性</font>。

### 批规范化层

批规范化层可以<font color='red'>加速收敛过程，提高训练过程中的稳定性</font>。

在没有对数据进行批规范化处理时，数据的分布是任意的，那么就会有大量的数据处在激活函数的敏感区域之外，而如果进行了数据规范化处理，相对来说数据的分布就比较均衡了。

在用卷积神经网络处理图像数据时，往往是几张图相同时输入网络进行前向计算，误差也是该批次中所有图像的误差累计起来一起回传。<font color='red'>批规范化方法其实就是对一个批次中的数据进行归一化处理</font>。

批规范化处理的流程：

> * $\mu\beta=\frac{1}{m}\sum_{i=1}^{m}x_i$,第一步，获得小批次的输入$\beta=\{x_1,...,x_m\}$，批次大小为$m$。
> * $\sigma_{\beta}^{2}=\frac{1}{m}\sum_{i=1}^{m}(x_i-\mu\beta)^2$，第二步求得这个批次的均值$\mu\beta$和方差$\sigma$。
> * $\widehat{x}_{i}=({x_i-\mu\beta})/{\sqrt{\sigma_{\beta}^{2}+\epsilon}}$，第三步，对所有的$x_i$进行标准化处理，得到$\widehat{x}_i$。这里的$\epsilon$是一个很小的值，避免分母为0的系统错误。
> * $y_i=\gamma\widehat{x}_i+\beta\equiv{BN_{\gamma,\beta}(x_i)}$ , 最后对$\widehat{x}$做线性变换，得到输出$y_i$。

上述流程中，<font color='red'>$\gamma和\beta$是可以学习的,神将网络会随着训练过程自己挑选一个最适合的分布</font>。

<font color='red'>批规范化处理通常在卷积层之后，激活函数之前</font>。虽然批规范化也不一定用在卷积层之后，但是一定用在激活函数之前，因为只有这样，才能发挥它的作用。

批规范化处理会在训练过程中调整每层网络输出数据的分布，使其能够合理进入激活函数作用区。

激活函数的作用区是指在原点附近的区域，梯度弥散率低，区分率高。

批规范化处理优点：

> * <font color='red'>加速收敛</font>，这是因为不必训练神经网络适应数据的分布。
> * 完美的使用激活函数对数据进行修剪，<font color='red'>减少了梯度弥散</font>。
> * 学习率可以变大。

### Dropout层

Dropout出现的原因：

>在机器学习的过程中，如果模型的参数太多，而训练样本太少，训练出来的模型很容易产生过拟合现象。
>
>过拟合具体表现为：<font color='red'>模型在训练集上损失函数较小，预测准确率高，而在测试集上损失函数较大，预测准确率低</font>。
>
>为了解决过拟合问题，一般会采用模型集成的方法，即训练多个模型进行组合，但训练多个模型很费时，测试多个模型也费时。
>
>Dropout的提出就是为了解决这一问题的。

Dropout在每次训练时，<font color='red'> 忽略一半的特征检测器（让一半的隐层节点值为0），可以明显的减少过拟合现象</font>。这种方式可以减少特征检测器之间的相互作用。

<font color='red'>补充：</font>检测器之间测相互作用是指某些检测器只能依赖其他检测器才能发挥作用。





